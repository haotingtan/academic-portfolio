{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c71d896-9cf4-46bd-97fb-9510e15220dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p6 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e3479a-55b0-47f4-8768-b23a92516780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address      Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.0.4  104.37 KiB  16      68.8%             859e4ddf-49f8-4514-a14e-ae44731c9bca  rack1\n",
      "UN  192.168.0.2  25.55 KiB   16      58.2%             3dc681e7-626d-45f8-9cc0-3726c0fef341  rack1\n",
      "UN  192.168.0.3  104.38 KiB  16      73.0%             7dcb8420-5885-4764-befc-4a4ec9d82536  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5e6984-3c1c-4f8f-8655-cf61d0875a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8550e2d-0500-452a-9384-3f3a61249996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fe3f815ef20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")\n",
    "cass.execute(\"create keyspace weather with replication={'class': 'SimpleStrategy', 'replication_factor': 3};\")\n",
    "cass.execute(\"use weather\")\n",
    "cass.execute(\"\"\"\n",
    "create type station_record (\n",
    "    tmin INT,\n",
    "    tmax INT\n",
    ")\n",
    "\"\"\")    \n",
    "cass.execute(\"drop table if exists stations\")\n",
    "cass.execute(\"\"\"\n",
    "CREATE TABLE stations (\n",
    "    id TEXT,\n",
    "    date DATE,\n",
    "    name TEXT STATIC,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY ((id), date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6808edbf-4b2e-4788-aa22-da18363df042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ae1a38-0acb-4ac2-944b-5ef6fde70e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-20dd3fca-fef0-4db2-aba2-5e9002f347b8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (115ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (298ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (59ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (36ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (114ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (35ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (33ms)\n",
      ":: resolution report :: resolve 5520ms :: artifacts dl 1209ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-20dd3fca-fef0-4db2-aba2-5e9002f347b8\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/93ms)\n",
      "23/11/20 07:01:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ee3468-0cb7-42eb-acb7-8e3242ec917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "df = df.withColumn(\"ID\", expr(\"substring(value, 1, 11)\"))\n",
    "df = df.withColumn(\"STATE\", expr(\"substring(value, 39, 2)\"))\n",
    "df = df.withColumn(\"NAME\", expr(\"substring(value, 42, 30)\"))\n",
    "df = df.filter(col(\"state\") == \"WI\")\n",
    "\n",
    "for row in df.collect():\n",
    "    name = row[\"NAME\"].replace(\"'\", \"''\")\n",
    "    name = name.strip()\n",
    "    cass.execute(f\"\"\"\n",
    "    INSERT INTO weather.stations (id, name)\n",
    "    VALUES ('{row[\"ID\"]}', '{name}')\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e02bb99-8fc7-4a0c-97c1-596669a50be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0   1313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(cass.execute(\"SELECT COUNT(*) FROM weather.stations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f531a6b-449a-4fd4-9c88-0b9a9c3b62e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MADISON DANE CO RGNL AP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "pd.DataFrame(cass.execute(\"select id, name from weather.stations WHERE id = 'USW00014837'\"))[\"name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f32613e-6d69-415e-8096-f1abfa9d5803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "q3 = pd.DataFrame(cass.execute(\"SELECT token(id) FROM weather.stations WHERE id = 'USC00470273'\"))[\"system_token_id\"][0]\n",
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ad9fa9-3e15-4765-8359-dc0f94d2e5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8982054996226954036"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "import subprocess\n",
    "import numpy\n",
    "\n",
    "result = subprocess.check_output(\n",
    "     \"nodetool ring\",\n",
    "     stderr=subprocess.STDOUT,\n",
    "     shell=True)\n",
    "result = result.decode(\"utf-8\")\n",
    "\n",
    "vnodes = []\n",
    "for line in result.splitlines():\n",
    "    tokens = line.strip().split()\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        vnodes.append(numpy.int64(tokens[-1]))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "vnodes = vnodes[1:]\n",
    "first_node = None\n",
    "for vnode in vnodes:\n",
    "    if vnode > q3:\n",
    "        first_node = vnode\n",
    "        break\n",
    "\n",
    "# wraps around\n",
    "if first_node == None:\n",
    "    first_node = vnode[0]\n",
    "\n",
    "first_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb38cf8-359b-4f73-9127-52a48c7918ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "parquet_df = []\n",
    "try:\n",
    "    with ZipFile('records.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    directory_path = 'records.parquet'\n",
    "\n",
    "    for path in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, path)\n",
    "        if path.strip().endswith('snappy.parquet'):\n",
    "            df = spark.read.parquet(file_path)\n",
    "            df = df.groupBy(\"station\", \"date\").pivot(\"element\", [\"TMAX\", \"TMIN\"]).sum(\"value\")\n",
    "            parquet_df.append(df)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "datas = []\n",
    "for df in parquet_df:\n",
    "    for row in df.collect():\n",
    "        station = str(row[\"station\"])\n",
    "        date = row[\"date\"][:4] + \"-\" + row[\"date\"][4:6] + \"-\" + row[\"date\"][6:]\n",
    "        tmin = int(row[\"TMIN\"])\n",
    "        tmax = int(row[\"TMAX\"])\n",
    "        datas.append([station, date, tmin, tmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5724d3ef-daf0-4ea7-92f1-69c162db83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "\n",
    "for data in datas:\n",
    "    request = station_pb2.RecordTempsRequest(station = data[0], \n",
    "                                             date = data[1],\n",
    "                                             tmin = data[2],\n",
    "                                             tmax = data[3])\n",
    "    response = stub.RecordTemps(request)\n",
    "    if response.error != \"\":\n",
    "        print(reponse.error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d082652b-7738-47f7-bee3-8e1d38bccba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "request = station_pb2.StationMaxRequest(station = 'USW00014837')\n",
    "response = stub.StationMax(request)\n",
    "if response.error != \"\":\n",
    "    print(response.error)\n",
    "response.tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44abff69-ddc0-480f-93b8-2a9568049fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"stations\")\n",
    "stations_df = (spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\n",
    ".option(\"keyspace\", \"weather\")\n",
    ".option(\"table\", \"stations\")\n",
    ".load())\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0601c5d7-18d0-48a3-a013-2daf0381e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee9f5f8-19fb-4e10-bb6e-75ed7ff49063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USW00014898': 102.93698630136986,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USW00014839': 89.6986301369863,\n",
       " 'USR0000WDDG': 102.06849315068493}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 07:03:40 WARN ChannelPool: [s0|p6-db-2/192.168.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=7b79b3b6-2dc4-40bf-801a-2bb6f43b1b45, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700463681701}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/20 07:03:47 WARN ChannelPool: [s0|p6-db-2/192.168.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=7b79b3b6-2dc4-40bf-801a-2bb6f43b1b45, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700463681701}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/20 07:03:57 WARN ChannelPool: [s0|p6-db-2/192.168.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=7b79b3b6-2dc4-40bf-801a-2bb6f43b1b45, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700463681701}): failed to send request (java.nio.channels.NotYetConnectedException))\n",
      "23/11/20 07:04:11 WARN ChannelPool: [s0|p6-db-2/192.168.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=7b79b3b6-2dc4-40bf-801a-2bb6f43b1b45, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700463681701}): failed to send request (java.nio.channels.NotYetConnectedException))\n"
     ]
    }
   ],
   "source": [
    "#q7\n",
    "q7 = spark.sql(\"\"\"\n",
    "SELECT id, AVG(record.tmax - record.tmin) as diff\n",
    "FROM stations\n",
    "WHERE record IS NOT NULL\n",
    "GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "output_dict = {}\n",
    "for row in q7.collect():\n",
    "    output_dict[row[0]] = row[1]\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fefdf3dd-c4b1-49df-800d-ae26562736af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address      Load       Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.0.4  86.05 KiB  16      100.0%            859e4ddf-49f8-4514-a14e-ae44731c9bca  rack1\n",
      "DN  192.168.0.2  86.04 KiB  16      100.0%            3dc681e7-626d-45f8-9cc0-3726c0fef341  rack1\n",
      "UN  192.168.0.3  95.88 KiB  16      100.0%            7dcb8420-5885-4764-befc-4a4ec9d82536  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd21290-d49c-4dca-8053-9f323cc27c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need 3 replicas, but only have 2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "request = station_pb2.StationMaxRequest(station = 'USW00014837')\n",
    "response = stub.StationMax(request)\n",
    "response.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e8539e-9227-4393-9f0f-24dee5ba32d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 07:04:30 WARN ChannelPool: [s0|p6-db-2/192.168.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=7b79b3b6-2dc4-40bf-801a-2bb6f43b1b45, APPLICATION_NAME=Spark-Cassandra-Connector-local-1700463681701}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n"
     ]
    }
   ],
   "source": [
    "#q10\n",
    "request = station_pb2.RecordTempsRequest(station = 'ABC00000001', \n",
    "                                             date = '2023-11-20',\n",
    "                                             tmin = -100,\n",
    "                                             tmax = 200)\n",
    "response = stub.RecordTemps(request)\n",
    "response.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
